import os
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"  # Desactiva optimizaciones OneDNN en TensorFlow para evitar problemas de compatibilidad.

from tensorflow.keras import datasets, layers, models, callbacks
import tensorflow as tf

# ğŸ“Œ Habilitar crecimiento dinÃ¡mico de memoria en GPU (evita errores por asignaciÃ³n de memoria fija)
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print(f"âœ… Crecimiento dinÃ¡mico de memoria habilitado para {len(gpus)} GPU(s).")
    except RuntimeError as e:
        print(e)

# ğŸ“¥ Cargar el dataset MNIST (dÃ­gitos escritos a mano)
# MNIST contiene imÃ¡genes en escala de grises de 28x28 pÃ­xeles con nÃºmeros del 0 al 9.
(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()

# ğŸ”„ Normalizar los datos a valores entre 0 y 1 para mejorar la convergencia del entrenamiento.
x_train, x_test = x_train / 255.0, x_test / 255.0

# ğŸ¯ ConstrucciÃ³n del modelo FFNN (Feedforward Neural Network)
model = models.Sequential([
    layers.Flatten(input_shape=(28, 28)),  # Convierte la imagen 28x28 en un vector de 784 elementos.
    layers.Dense(128, activation="relu"),  # Capa oculta con 128 neuronas y activaciÃ³n ReLU (detecta patrones en la imagen).
    layers.Dense(64, activation="relu"),   # Segunda capa oculta con 64 neuronas para refinar la representaciÃ³n.
    layers.Dense(10, activation="softmax") # Capa de salida con 10 neuronas (una por cada dÃ­gito 0-9), activaciÃ³n softmax.
])

# ğŸ› ï¸ Compilar el modelo
model.compile(
    optimizer="adam",  # Algoritmo de optimizaciÃ³n basado en gradiente descendente adaptativo.
    loss="sparse_categorical_crossentropy",  # FunciÃ³n de pÃ©rdida para clasificaciÃ³n con etiquetas enteras (0-9).
    metrics=["accuracy"]  # Seguimiento de la precisiÃ³n durante el entrenamiento.
)

# â³ ConfiguraciÃ³n de Early Stopping
# Detiene el entrenamiento si la pÃ©rdida en validaciÃ³n no mejora en 5 Ã©pocas consecutivas.
early_stop = callbacks.EarlyStopping(
    monitor='val_loss',    # Observa la funciÃ³n de pÃ©rdida en validaciÃ³n.
    patience=5,            # Espera 5 Ã©pocas antes de detenerse si no hay mejoras.
    restore_best_weights=True  # Restaura los mejores pesos encontrados antes de la detenciÃ³n.
)

# ğŸš€ Entrenamiento del modelo usando GPU (si estÃ¡ disponible) con Early Stopping
with tf.device('/GPU:0'):
    model.fit(
        x_train, y_train,
        epochs=30,  # Entrena el modelo durante un mÃ¡ximo de 30 Ã©pocas.
        validation_split=0.2,  # Usa el 20% de los datos de entrenamiento para validaciÃ³n.
        callbacks=[early_stop]  # Usa el mecanismo de Early Stopping.
    )

# ğŸ“Š Evaluar el modelo con los datos de prueba (test)
loss, accuracy = model.evaluate(x_test, y_test)
print(f"ğŸ“‰ PÃ©rdida: {loss:.4f}, ğŸ¯ PrecisiÃ³n: {accuracy:.4f}")

# ğŸ’¾ Guardar el modelo entrenado en formato Keras
model.save("char_recognizer_model_ffnn.keras")
print("âœ… Modelo guardado como 'char_recognizer_model_ffnn.keras'")
